{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900165b5-ef9b-4c58-b577-f07cfbab8cbb",
   "metadata": {},
   "source": [
    "# Text Generation with LLaMA-3\n",
    "\n",
    "This notebook demonstrates how to use the LLaMA-3 model from Hugging Face for text generation. We'll set up the model, perform text generation, and evaluate the results. \n",
    "Having issues wit git. \n",
    "Setting up in vscode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088eb42-bf7b-4517-906a-5cf3aacdb7ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, login, HfApi\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "# notebook_login()\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "# Define the model ID and load the model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id,\n",
    "                                 model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "                                 device_map=\"auto\")\n",
    "\n",
    "# Test the pipeline with a sample input\n",
    "result = pipeline(\"Hey, how are you doing today?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56faabd6-583f-418d-91e9-59a67e59e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The quick brown fox jumps over\",\n",
    "    \"In a world where technology and nature coexist\",\n",
    "]\n",
    "\n",
    "# Generate and print results for each prompt\n",
    "for prompt in prompts:\n",
    "    result = pipeline(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {result[0]['generated_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36b1c7-7640-46c8-a919-8c15ba32265f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to use the LLaMA-3 model from Hugging Face for text generation. We set up the model, provided various prompts, and observed the generated text. The model shows a remarkable ability to generate coherent and contextually relevant text based on the input prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637c738-d814-4ff1-8d02-a11f73f01632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7abded8-33b3-4d01-b80d-d99a3848ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
